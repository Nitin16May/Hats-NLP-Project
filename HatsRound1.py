# -*- coding: utf-8 -*-
"""NLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nYafou7Cpkx69CmggHj6Fa79DKrrOo5H
"""

!pip install pattern
import nltk
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

import operator
import nltk
from matplotlib import pyplot as plt
import string
from nltk.stem import PorterStemmer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet
from collections import Counter, defaultdict
import seaborn as sns
import pandas as pd
import numpy as np
from wordcloud import WordCloud, STOPWORDS
from pattern.text.en import singularize
import inflect

# Read book from a file
t1 = open("book1.txt", encoding="utf8")

# Initialize lists and objects
text = []
words = []
final = []
nouns = []
verbs = []
lemmatizer = WordNetLemmatizer()
custom = ["chapter", "page"]
p = inflect.engine()

# Helper function to map Treebank POS tags to WordNet POS tags
def get_wordnet_pos(treebank_tag):
    if treebank_tag.startswith('J'):
        return wordnet.ADJ
    elif treebank_tag.startswith('V'):
        return wordnet.VERB
    elif treebank_tag.startswith('N'):
        return wordnet.NOUN
    elif treebank_tag.startswith('R'):
        return wordnet.ADV
    else:
        return wordnet.NOUN

# Read sentences from the book
for line in t1:
    text.append(line.strip())

# Process and tokenize sentences
for sentence in text:
    # Remove punctuation and digits
    punctuationfree = "".join([i for i in sentence if i not in string.punctuation and not i.isdigit()])
    # Tokenize words
    word_tokens = word_tokenize(punctuationfree)
    # Perform part of speech tagging
    filtered_sentence = nltk.pos_tag(word_tokens)
    # Filter and preprocess words, removing stopwords and applying singularization
    filtered_sentence = [(singularize(w.casefold()), t) for w, t in filtered_sentence if not w.lower() in STOPWORDS and len(w) > 1 and w.lower() not in custom]
    words.append(filtered_sentence)

# Lemmatize words based on their POS tags
for sentence in words:
    temp = []
    for word, tag in sentence:
        temp.append((lemmatizer.lemmatize(word, get_wordnet_pos(tag)), tag))
    final.append(temp)

# Plotting Word frequency
to_plot = []
for sentence in words:
    for word, tag in sentence:
        to_plot.append(word)

counted = Counter(to_plot)
word_freq = pd.DataFrame(counted.items(), columns=['word', 'frequency']).sort_values(by='frequency', ascending=False)

word_freq = word_freq.head(25)
plt.title("Word frequency")
sns.barplot(x='frequency', y='word', data=word_freq)
plt.show()

# Combine the words in 'final' into a single string
word_text = ' '.join([word for sentence in final for word, _ in sentence])

# Create a WordCloud object without custom stopwords
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(word_text)

# Display the word cloud using matplotlib
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title("Word Cloud")
plt.show()

# Plotting tag frequency (Treebank Tags)
to_plot = []
to_plot_tag = []
for sentence in words:
    for word, tag in sentence:
        to_plot_tag.append(tag)

counted_tag = Counter(to_plot_tag)
tag_freq = pd.DataFrame(counted_tag.items(), columns=['tag', 'frequency']).sort_values(by='frequency', ascending=False)

tag_freq = tag_freq.head(10)
plt.title("Tag Frequency (Treebank Tags)")
sns.barplot(x='frequency', y='tag', data=tag_freq)
plt.show()

import re
from collections import defaultdict

def get_largest_chapter(file_path):
    with open('book1.txt', 'r') as file:
        data = file.read()

    # Assuming chapters are separated by 'Chapter {number}'
    chapters = re.split(r'Chapter', data)[39:]
    # Store chapter lengths
    chapter_lengths = defaultdict(int)
    global corpus
    corpus = chapters[9]
    for i, chapter in enumerate(chapters):
        # Remove whitespace and count words
        words = re.findall(r'\b\w+\b', chapter)
        chapter_lengths[i+1] = len(words)

    # Get chapter with maximum length
    largest_chapter = max(chapter_lengths, key=chapter_lengths.get)

    return largest_chapter, chapter_lengths[largest_chapter]

# Usage
file_path = 'book1.txt'
largest_chapter, length = get_largest_chapter(file_path)
print(f'The largest chapter is Chapter {largest_chapter} with {length} words.')

def preprocesscorpus( corpus):
  corpus = corpus.lower()
  corpus = "eos " + corpus
  corpus = corpus.replace("."," eos" )
  corpus = corpus.replace("!"," eos" )
  corpus = corpus.replace("?"," eos" )
  corpus = corpus.replace(",", "")
  corpus = corpus.replace("“", "")
  corpus = corpus.replace("”", "")
  corpus = corpus.replace(";", "")
  corpus = corpus.replace("-", " ")
  corpus = corpus.replace("0", "")
  corpus = corpus.replace("1", "")
  corpus = corpus.replace("2", "")
  corpus = corpus.replace("3", "")
  corpus = corpus.replace("4", "")
  corpus = corpus.replace("5", "")
  corpus = corpus.replace("6", "")
  corpus = corpus.replace("7", "")
  corpus = corpus.replace("8", "")
  corpus = corpus.replace("9", "")
  corpus = corpus.replace("’s", "")
  return corpus

corpus = preprocesscorpus(corpus)
print(corpus)

from nltk import word_tokenize
def generate_token(corpus):
  tokens = word_tokenize(corpus)
  return tokens
tokens = generate_token(corpus)
dis_token = list(set(sorted(tokens)))
print(dis_token)

def freq(tokens):
  dic={}
  for tok in tokens:
    dic[tok]=0
  for tok in tokens:
    dic[tok]+=1
  return dic
dic = freq(tokens)
for i in dic.items():
  print(i[0],"\t:",i[1])

def gen_bigram(tokens,k):
    l=[]
    i=0
    while(i<len(tokens)):
        l.append(tokens[i:i+k])
        i=i+1
    l=l[:-1]
    return l
bigram = gen_bigram(tokens,2)
for i in bigram:
    print(i)

def gen_bigram_freq(bigram):
    dct1={}
    for i in bigram:
        st=" ".join(i)
        dct1[st]=0
    for i in bigram:
        st=" ".join(i)
        dct1[st]+=1
    return dct1
dct1=gen_bigram_freq(bigram)
for i in dct1.items():
    print(i[0], ":", i[1])

def find1(s,dct1):
    try:
        return dct1[s]
    except:
        return 0
def print_probability_table(distinct_tokens,dct,dct1):
    n=len(distinct_tokens)
    l=[[]*n for i in range(n)]
    for i in range(n):
        denominator = dct[distinct_tokens[i]]
        for j in range(n):
            numerator = find1(distinct_tokens[i]+" "+distinct_tokens[j],dct1)
            l[i].append(float("{:.3f}".format(numerator/denominator)))
    return l

print("Number of tokens = \n")
probability_table=print_probability_table(dis_token,dic,dct1)
n=len(dis_token)
print(n)
# print("\t", end="")
# for i in range(n):
#     print(dis_token[i],end="\t")
# print("\n")
# for i in range(n):
#     print(dis_token[i],end="\t")
#     for j in range(n):
#         print(probability_table[i][j],end="\t")
#     print("\n")

def generate_next_word(sentence, probability_table, dis_token):
    if sentence:
        last_word = sentence.split()[-1]
        if last_word in dis_token:
            last_word_index = dis_token.index(last_word)
            probabilities = probability_table[last_word_index]
            if probabilities:
                most_likely_word_index = probabilities.index(max(probabilities))
                most_likely_word = dis_token[most_likely_word_index]
                return most_likely_word
    return None

# Example usage:
sentence = input("Enter a sentence: ")
next_word = generate_next_word(sentence, probability_table, dis_token)
if next_word:
    print(f"The next most probable word after '{sentence}' is: {next_word}")
else:
    print("No probable next word found.")

def generate_next_word(sentence, probability_table, dis_token):
    if sentence:
        last_word = sentence.split()[-1]
        if last_word in dis_token:
            last_word_index = dis_token.index(last_word)
            probabilities = probability_table[last_word_index]
            if probabilities:
                most_likely_word_index = probabilities.index(max(probabilities))
                most_likely_word = dis_token[most_likely_word_index]
                return most_likely_word
    return None

def play_shannon_game(original_sentence, probability_table, dis_token):
    words = original_sentence.split()
    generated_sentence = []

    for word in words:
        generated_sentence.append(word)
        next_word = generate_next_word(" ".join(generated_sentence), probability_table, dis_token)
        if next_word:
            generated_sentence.append(next_word)

    generated_sentence = " ".join(generated_sentence)
    return generated_sentence

# Example usage:
original_sentence = "which seemed to proclaim that the whole army of the prince was about to emerge from the mountain passes"
generated_sentence = play_shannon_game(original_sentence, probability_table, dis_token)

print("Original Sentence:", original_sentence)
print("Generated Sentence:", generated_sentence)